{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "The code snippet imports several libraries required for data processing, machine learning, and deep learning.\n",
    "\n",
    "1. `itertools.chain` and `itertools.combinations`:\n",
    "   - These functions from the `itertools` module are imported to handle iterable combinations.\n",
    "\n",
    "2. `pandas`:\n",
    "   - The `pandas` library is imported as `pd` to provide data manipulation and analysis capabilities.\n",
    "\n",
    "3. `numpy`:\n",
    "   - The `numpy` library is imported as `np` to support numerical operations and array manipulation.\n",
    "\n",
    "4. `yfinance`:\n",
    "   - The `yfinance` library is imported as `yf` to fetch financial market data from Yahoo Finance.\n",
    "\n",
    "5. `sklearn.preprocessing.StandardScaler`:\n",
    "   - The `StandardScaler` class from the `sklearn.preprocessing` module is imported to standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "6. `sklearn.model_selection.train_test_split` and `sklearn.model_selection.cross_val_score`:\n",
    "   - These functions from the `sklearn.model_selection` module are imported to perform data splitting for training and testing, as well as cross-validation.\n",
    "\n",
    "7. `sklearn.linear_model.LogisticRegression`:\n",
    "   - The `LogisticRegression` class from the `sklearn.linear_model` module is imported to create a logistic regression model.\n",
    "\n",
    "8. `sklearn.metrics`:\n",
    "   - The `sklearn.metrics` module is imported as `metrics` to provide various evaluation metrics for model performance.\n",
    "\n",
    "9. `sklearn.metrics.accuracy_score`, `sklearn.metrics.precision_score`, `sklearn.metrics.recall_score`, `sklearn.metrics.f1_score`, `sklearn.metrics.roc_auc_score`, `sklearn.metrics.confusion_matrix`:\n",
    "   - These functions from the `sklearn.metrics` module are imported to calculate various evaluation metrics such as accuracy, precision, recall, F1 score, ROC AUC score, and confusion matrix.\n",
    "\n",
    "10. `sklearn.impute.SimpleImputer`:\n",
    "    - The `SimpleImputer` class from the `sklearn.impute` module is imported to handle missing values in the data.\n",
    "\n",
    "11. `tensorflow.keras.models.Sequential`, `tensorflow.keras.layers.LSTM`, `tensorflow.keras.layers.Dense`, `tensorflow.keras.layers.Dropout`, `tensorflow.keras.layers.BatchNormalization`:\n",
    "    - These classes from the `tensorflow.keras.models` and `tensorflow.keras.layers` modules are imported to build a sequential model with LSTM layers, dense layers, dropout layers, and batch normalization layers.\n",
    "\n",
    "12. `tensorflow.keras.optimizers.Adam`:\n",
    "    - The `Adam` class from the `tensorflow.keras.optimizers` module is imported to define the Adam optimizer for model training.\n",
    "\n",
    "By importing these libraries, the code snippet prepares the necessary tools for data processing, machine learning, and deep learning tasks.\n",
    "\n",
    "Please note that you may need to install some of these libraries using the appropriate package manager before executing this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from itertools import chain, combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Cryptocurrency Data\n",
    "\n",
    "To download data for a specific cryptocurrency, the following code snippet utilizes the `yfinance` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# download data of the cryptocurrency\n",
    "\n",
    "df_xmr = yf.download(tickers=\"XMR-USD\", period=\"max\", interval=\"1d\", start=\"2023-01-01\", end=\"2023-10-09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>147.309662</td>\n",
       "      <td>148.931030</td>\n",
       "      <td>146.437485</td>\n",
       "      <td>148.576935</td>\n",
       "      <td>148.576935</td>\n",
       "      <td>36453347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>148.582184</td>\n",
       "      <td>149.623535</td>\n",
       "      <td>147.943558</td>\n",
       "      <td>147.943558</td>\n",
       "      <td>147.943558</td>\n",
       "      <td>47050925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>147.933929</td>\n",
       "      <td>149.027832</td>\n",
       "      <td>147.628860</td>\n",
       "      <td>148.487930</td>\n",
       "      <td>148.487930</td>\n",
       "      <td>48662135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>148.466995</td>\n",
       "      <td>152.488983</td>\n",
       "      <td>148.342621</td>\n",
       "      <td>150.743652</td>\n",
       "      <td>150.743652</td>\n",
       "      <td>83915181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>150.790253</td>\n",
       "      <td>155.921738</td>\n",
       "      <td>150.769043</td>\n",
       "      <td>155.921738</td>\n",
       "      <td>155.921738</td>\n",
       "      <td>78049428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2023-01-01  147.309662  148.931030  146.437485  148.576935  148.576935   \n",
       "2023-01-02  148.582184  149.623535  147.943558  147.943558  147.943558   \n",
       "2023-01-03  147.933929  149.027832  147.628860  148.487930  148.487930   \n",
       "2023-01-04  148.466995  152.488983  148.342621  150.743652  150.743652   \n",
       "2023-01-05  150.790253  155.921738  150.769043  155.921738  155.921738   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2023-01-01  36453347  \n",
       "2023-01-02  47050925  \n",
       "2023-01-03  48662135  \n",
       "2023-01-04  83915181  \n",
       "2023-01-05  78049428  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the downloaded data\n",
    "df_xmr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation and Feature Engineering\n",
    "\n",
    "This code snippet demonstrates data manipulation and feature engineering steps performed on the downloaded cryptocurrency data.\n",
    "\n",
    "1. Calculating the Actual Class Labels:\n",
    "   - The code compares the closing prices of the \"XMR-USD\" cryptocurrency from January 4, 2023, onwards with the closing prices between January 3, 2023, and October 7, 2023.\n",
    "   - The comparison is done element-wise, and the resulting boolean values are converted to binary values (0 or 1) using `astype(int)`.\n",
    "   - The calculated class labels representing price increase or decrease are stored in the `actual_class` variable.\n",
    "\n",
    "2. Dropping a Specific Date:\n",
    "   - The code removes a specific date, \"2023-10-08\", from the `df_xmr` dataframe.\n",
    "   - This operation eliminates the corresponding row from the dataframe.\n",
    "\n",
    "3. Creating a New Feature: \"Price increase (in the next day)\":\n",
    "   - The code adds a new feature called \"Price increase (in the next day)\" to the `df_xmr` dataframe.\n",
    "   - The feature is populated with the values from the `actual_class` variable, representing the price increase (1) or decrease (0) in the subsequent day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_class = (\n",
    "    (df_xmr.loc[\"2023-01-04\":, \"Close\"].to_numpy() > df_xmr.loc[\"2023-01-03\":\"2023-10-07\", \"Close\"]).astype(int)\n",
    ")\n",
    "df_xmr = df_xmr.drop(\"2023-10-08\")\n",
    "df_xmr[\"Price increase (in the next day)\"] = actual_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data into Training, Validation, and Test Sets\n",
    "\n",
    "The code splits the data into training, validation, and test sets to evaluate the model's performance and assess its generalizability. The `train_test_split` function from the `sklearn.model_selection` module is used for this purpose.\n",
    "\n",
    "1. Initial Split:\n",
    "   - The `df_xmr` dataframe is initially split into two parts: `train_data` and `test_data`.\n",
    "   - The `test_data` portion is 20% of the original data, while the `train_data` portion contains the remaining 80% of the data.\n",
    "\n",
    "2. Further Splitting:\n",
    "   - The `train_data` portion is further split into two parts: `train_data` and `val_data`.\n",
    "   - The `val_data` portion is also 20% of the `train_data`, while the `train_data` retains the remaining 80% of the data.\n",
    "\n",
    "By splitting the data into training, validation, and test sets, it allows for different stages of model development and evaluation:\n",
    "- The `train_data` set is used for training the model.\n",
    "- The `val_data` set is utilized for hyperparameter tuning and assessing the model's performance.\n",
    "- The `test_data` set serves as an independent dataset to evaluate the final model's generalization ability and performance.\n",
    "\n",
    "Please note that setting `shuffle=False` ensures that the data is split sequentially, maintaining the temporal order of the data. This can be important in scenarios such as time-series analysis or any other case where the order of the data is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(df_xmr, test_size=0.2, shuffle=False, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Feature Columns and Target Variable\n",
    "\n",
    "The code snippet defines the feature columns and target variable for the model training and prediction.\n",
    "\n",
    "1. Feature Columns:\n",
    "   - The `feature_cols` list specifies the names of the columns from the dataset that will serve as input features for the model.\n",
    "   - In this case, the feature columns include `'Open'`, `'High'`, `'Low'`, `'Close'`, `'Adj Close'`, and `'Volume'`.\n",
    "\n",
    "2. Target Variable:\n",
    "   - The `target_col` variable represents the target variable or the column that the model aims to predict.\n",
    "   - In this case, the target variable is `'Price increase (in the next day)'`.\n",
    "\n",
    "By defining the feature columns and target variable, the code establishes which features will be used as inputs and which column the model will predict during the training and evaluation process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and target variable\n",
    "feature_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "target_col = 'Price increase (in the next day)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Features and Target Variable for Training, Validation, and Test Sets\n",
    "\n",
    "The code snippet splits the features and target variable into separate variables for the training, validation, and test sets. This step is crucial for preparing the data for model training and evaluation.\n",
    "\n",
    "1. Training Set:\n",
    "   - The `X_train` variable contains the features for the training set.\n",
    "   - The features are obtained from the `train_data` dataframe using the specified `feature_cols`.\n",
    "   - The `y_train` variable represents the corresponding target variable for the training set.\n",
    "   - The target variable is obtained from the `train_data` dataframe using the specified `target_col`.\n",
    "\n",
    "2. Validation Set:\n",
    "   - The `X_val` variable contains the features for the validation set.\n",
    "   - The features are obtained from the `val_data` dataframe using the specified `feature_cols`.\n",
    "   - The `y_val` variable represents the corresponding target variable for the validation set.\n",
    "   - The target variable is obtained from the `val_data` dataframe using the specified `target_col`.\n",
    "\n",
    "3. Test Set:\n",
    "   - The `X_test` variable contains the features for the test set.\n",
    "   - The features are obtained from the `test_data` dataframe using the specified `feature_cols`.\n",
    "   - The `y_test` variable represents the corresponding target variable for the test set.\n",
    "   - The target variable is obtained from the `test_data` dataframe using the specified `target_col`.\n",
    "\n",
    "By splitting the features and target variable for each set, the code prepares the data in separate variables for model training and evaluation on the training, validation, and test sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target variable for training, validation, and test sets\n",
    "X_train, y_train = train_data[feature_cols], train_data[target_col]\n",
    "X_val, y_val = val_data[feature_cols], val_data[target_col]\n",
    "X_test, y_test = test_data[feature_cols], test_data[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values in the Features and Target Variables\n",
    "\n",
    "The code snippet handles missing values in the features and target variables using the `SimpleImputer` class from the `sklearn.impute` module.\n",
    "\n",
    "1. Creating an Imputer:\n",
    "   - An instance of the `SimpleImputer` class is created with the `strategy` parameter set to `\"mean\"`.\n",
    "   - The `\"mean\"` strategy replaces missing values with the mean of the non-missing values in the respective column.\n",
    "\n",
    "2. Handling Missing Values:\n",
    "   - The missing values in the training set, `X_train`, are handled by fitting the imputer on the training data using the `fit_transform()` method.\n",
    "   - The missing values in the validation set, `X_val`, are handled by transforming the validation data using the `transform()` method.\n",
    "   - The missing values in the test set, `X_test`, are also handled by transforming the test data using the `transform()` method.\n",
    "\n",
    "By applying the `SimpleImputer` with the mean strategy, the code replaces missing values in the features with the mean of the non-missing values in each respective column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in the features and target variables\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet scales the features using the `StandardScaler` from the `sklearn.preprocessing` module. \n",
    "\n",
    "The `StandardScaler` is a preprocessing class used to standardize features by removing the mean and scaling to unit variance. This normalization technique is commonly applied to ensure that all features have a similar scale, which can be beneficial for many machine learning algorithms.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. A `StandardScaler` object is created and assigned to the variable `scaler`.\n",
    "2. The `fit_transform()` method is called on the `scaler` object with `X_train` as the input. This step fits the scaler to the training data and then transforms it.\n",
    "3. The `transform()` method is called on the `scaler` object with `X_val` and `X_test` as inputs. This step applies the previously fitted scaler to transform the validation and test data.\n",
    "\n",
    "By executing these steps, the code scales the features of the `X_train`, `X_val`, and `X_test` datasets using the `StandardScaler`.\n",
    "\n",
    "Scaling the features is often necessary to ensure that they are on a similar scale, which can improve the performance of machine learning models that rely on distance-based calculations or when using optimization algorithms that are sensitive to feature scales.\n",
    "\n",
    "Please note that before scaling the features, make sure that the data is in a numerical format and that the features are appropriately prepared for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values in the Target Variable\n",
    "\n",
    "The code snippet handles missing values in the target variable using the `SimpleImputer` class from the `sklearn.impute` module.\n",
    "\n",
    "1. Creating an Imputer:\n",
    "   - An instance of the `SimpleImputer` class is created with the `strategy` parameter set to `\"most_frequent\"`.\n",
    "   - The `\"most_frequent\"` strategy replaces missing values with the most frequent value in the respective column.\n",
    "\n",
    "2. Handling Missing Values:\n",
    "   - Missing values in the training set target variable, `y_train`, are handled by fitting the imputer on the training target data using the `fit_transform()` method.\n",
    "   - Missing values in the validation set target variable, `y_val`, are handled by transforming the validation target data using the `transform()` method.\n",
    "   - Missing values in the test set target variable, `y_test`, are also handled by transforming the test target data using the `transform()` method.\n",
    "\n",
    "By applying the `SimpleImputer` with the most frequent strategy, the code replaces missing values in the target variable with the most frequent value in the respective column.\n",
    "\n",
    "Handling missing values in the target variable is important to ensure that the data is complete and suitable for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in the target variable\n",
    "imputer_target = SimpleImputer(strategy=\"most_frequent\")\n",
    "y_train = imputer_target.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_val = imputer_target.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "y_test = imputer_target.transform(y_test.values.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet reshapes the features into a 3D format `(samples, timesteps, features)`. This reshaping is commonly required when working with certain types of deep learning models, such as recurrent neural networks (RNNs) or LSTM (Long Short-Term Memory) models.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. Reshaping `X_train`:\n",
    "   - The `np.reshape()` function from the `numpy` library is used to reshape `X_train`.\n",
    "   - The shape of `X_train` is `(X_train.shape[0], 1, X_train.shape[1])`, where:\n",
    "     - `X_train.shape[0]` represents the number of samples.\n",
    "     - `1` represents the number of timesteps.\n",
    "     - `X_train.shape[1]` represents the number of features.\n",
    "   - This reshaping converts `X_train` into a 3D array.\n",
    "\n",
    "2. Reshaping `X_val`:\n",
    "   - The same approach is applied to reshape `X_val`.\n",
    "   - The shape of `X_val` becomes `(X_val.shape[0], 1, X_val.shape[1])`.\n",
    "\n",
    "3. Reshaping `X_test`:\n",
    "   - Similarly, `X_test` is reshaped using the same approach.\n",
    "   - The shape of `X_test` becomes `(X_test.shape[0], 1, X_test.shape[1])`.\n",
    "\n",
    "By executing these steps, the code reshapes the features of `X_train`, `X_val`, and `X_test` into a 3D format, where the dimensions represent samples, timesteps, and features, respectively.\n",
    "\n",
    "Please note that before reshaping the features, ensure that the data is appropriately prepared and preprocessed for the intended deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features into 3D format (samples, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet defines an LSTM (Long Short-Term Memory) model using the Keras framework.\n",
    "\n",
    "The code performs the following steps to define the LSTM model:\n",
    "\n",
    "1. Creating the Model:\n",
    "   - An instance of the `Sequential` class is created and assigned to the variable `model`. The `Sequential` class represents a linear stack of layers in the neural network.\n",
    "\n",
    "2. Adding LSTM Layers:\n",
    "   - The first LSTM layer is added to the model using the `model.add()` method.\n",
    "   - The layer has 128 units (neurons) and is configured to return sequences using `return_sequences=True`.\n",
    "   - The `input_shape` parameter is set to `(X_train.shape[1], X_train.shape[2])`, which corresponds to the shape of the input data.\n",
    "\n",
    "3. Adding Dropout and Batch Normalization:\n",
    "   - After the first LSTM layer, a Dropout layer with a rate of 0.2 is added using `model.add(Dropout(0.2))`. Dropout helps prevent overfitting by randomly setting a fraction of input units to 0 during training.\n",
    "   - A Batch Normalization layer is added using `model.add(BatchNormalization())`. Batch Normalization normalizes the activations of the previous layer, helping stabilize the learning process.\n",
    "\n",
    "4. Adding the Second LSTM Layer:\n",
    "   - Another LSTM layer is added to the model using `model.add(LSTM(64))`. This layer has 64 units.\n",
    "   - Similarly, a Dropout layer with a rate of 0.2 and a Batch Normalization layer are added after the second LSTM layer.\n",
    "\n",
    "5. Adding the Output Layer:\n",
    "   - The final layer is a Dense layer with 1 unit and a sigmoid activation function, added using `model.add(Dense(1, activation='sigmoid'))`. This layer produces the output of the model.\n",
    "\n",
    "By executing these steps, the code defines an LSTM model with two LSTM layers, dropout layers, batch normalization layers, and a dense output layer. The architecture and hyperparameters of the LSTM model can be modified based on the specific requirements of the problem and the dataset being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet compiles the LSTM model by specifying the optimizer, loss function, and evaluation metrics.\n",
    "\n",
    "The code performs the following steps to compile the model:\n",
    "\n",
    "1. Specifying the Optimizer:\n",
    "   - An instance of the Adam optimizer is created and assigned to the variable `optimizer`. The `Adam` optimizer is a popular optimization algorithm used for training deep learning models.\n",
    "   - The learning rate of the optimizer is set to `0.001` using the `learning_rate` parameter.\n",
    "\n",
    "2. Specifying the Loss Function:\n",
    "   - The loss function for the model is specified as `'binary_crossentropy'` using the `loss` parameter. This loss function is commonly used for binary classification problems.\n",
    "\n",
    "3. Specifying Evaluation Metrics:\n",
    "   - The evaluation metric for the model is specified as `['accuracy']` using the `metrics` parameter. This metric measures the accuracy of the model during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet trains the LSTM model using the provided training data.\n",
    "\n",
    "The code performs the following steps to train the model:\n",
    "\n",
    "1. Training Data:\n",
    "   - The training data is passed to the `model.fit()` method.\n",
    "   - `X_train` represents the input features of the training data.\n",
    "   - `y_train` represents the target labels of the training data.\n",
    "\n",
    "2. Training Parameters:\n",
    "   - The `epochs` parameter is set to `50`, indicating the number of times the model will iterate over the entire training dataset during training.\n",
    "   - The `batch_size` parameter is set to `32`, specifying the number of samples that will be propagated through the network at once.\n",
    "   - The `verbose` parameter is set to `1`, which determines the verbosity mode. In this case, `1` indicates that progress updates will be displayed during training.\n",
    "\n",
    "3. Validation Data:\n",
    "   - The validation data is provided using the `validation_data` parameter.\n",
    "   - `X_val` represents the input features of the validation data.\n",
    "   - `y_val` represents the target labels of the validation data.\n",
    "\n",
    "By executing these steps, the code trains the LSTM model using the provided training data, with the specified training parameters and validation data. The model will iterate over the training dataset for 50 epochs, updating its weights based on the defined loss function and optimizer. The training progress will be displayed due to the verbose mode set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 5s 183ms/step - loss: 0.7997 - accuracy: 0.5140 - val_loss: 0.6933 - val_accuracy: 0.5111\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7664 - accuracy: 0.5475 - val_loss: 0.6926 - val_accuracy: 0.5111\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7457 - accuracy: 0.5251 - val_loss: 0.6928 - val_accuracy: 0.5111\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7910 - accuracy: 0.4916 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7479 - accuracy: 0.5754 - val_loss: 0.6937 - val_accuracy: 0.5111\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6928 - accuracy: 0.6257 - val_loss: 0.6941 - val_accuracy: 0.5111\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7049 - accuracy: 0.5531 - val_loss: 0.6944 - val_accuracy: 0.5111\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6508 - accuracy: 0.6089 - val_loss: 0.6947 - val_accuracy: 0.5111\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.7027 - accuracy: 0.5587 - val_loss: 0.6951 - val_accuracy: 0.5111\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6550 - accuracy: 0.5587 - val_loss: 0.6955 - val_accuracy: 0.5111\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6637 - accuracy: 0.5978 - val_loss: 0.6955 - val_accuracy: 0.5111\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.7096 - accuracy: 0.5642 - val_loss: 0.6956 - val_accuracy: 0.5111\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6702 - accuracy: 0.6034 - val_loss: 0.6964 - val_accuracy: 0.5111\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6875 - accuracy: 0.5587 - val_loss: 0.6968 - val_accuracy: 0.5111\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6889 - accuracy: 0.5754 - val_loss: 0.6975 - val_accuracy: 0.5111\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7021 - accuracy: 0.5754 - val_loss: 0.6975 - val_accuracy: 0.5111\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6694 - accuracy: 0.5866 - val_loss: 0.6978 - val_accuracy: 0.5111\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.7175 - accuracy: 0.5866 - val_loss: 0.6978 - val_accuracy: 0.5111\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6823 - accuracy: 0.6034 - val_loss: 0.6977 - val_accuracy: 0.5111\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6829 - accuracy: 0.5978 - val_loss: 0.6969 - val_accuracy: 0.5111\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6937 - accuracy: 0.5531 - val_loss: 0.6966 - val_accuracy: 0.5111\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.7112 - accuracy: 0.5810 - val_loss: 0.6958 - val_accuracy: 0.5111\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6861 - accuracy: 0.5978 - val_loss: 0.6965 - val_accuracy: 0.5111\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6567 - accuracy: 0.6089 - val_loss: 0.6978 - val_accuracy: 0.5111\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7286 - accuracy: 0.5587 - val_loss: 0.6978 - val_accuracy: 0.5111\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6873 - accuracy: 0.6145 - val_loss: 0.6966 - val_accuracy: 0.5111\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6636 - accuracy: 0.6425 - val_loss: 0.6961 - val_accuracy: 0.5111\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6682 - accuracy: 0.5810 - val_loss: 0.6976 - val_accuracy: 0.5111\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6777 - accuracy: 0.5642 - val_loss: 0.6965 - val_accuracy: 0.5111\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6111 - accuracy: 0.6592 - val_loss: 0.6960 - val_accuracy: 0.5111\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6841 - accuracy: 0.5922 - val_loss: 0.6971 - val_accuracy: 0.5111\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6351 - accuracy: 0.6201 - val_loss: 0.6982 - val_accuracy: 0.5111\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6598 - accuracy: 0.6201 - val_loss: 0.6968 - val_accuracy: 0.5111\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.7000 - accuracy: 0.5642 - val_loss: 0.6975 - val_accuracy: 0.5111\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6887 - accuracy: 0.6089 - val_loss: 0.6974 - val_accuracy: 0.5111\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6632 - accuracy: 0.6034 - val_loss: 0.6979 - val_accuracy: 0.5111\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6518 - accuracy: 0.5978 - val_loss: 0.6993 - val_accuracy: 0.5111\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6652 - accuracy: 0.5698 - val_loss: 0.6976 - val_accuracy: 0.5111\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6126 - accuracy: 0.6648 - val_loss: 0.6981 - val_accuracy: 0.5111\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6295 - accuracy: 0.5866 - val_loss: 0.6987 - val_accuracy: 0.5111\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.6195 - accuracy: 0.6034 - val_loss: 0.6980 - val_accuracy: 0.5111\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.6332 - accuracy: 0.6034 - val_loss: 0.6965 - val_accuracy: 0.4889\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6419 - accuracy: 0.6145 - val_loss: 0.6974 - val_accuracy: 0.4667\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.6509 - accuracy: 0.6480 - val_loss: 0.6955 - val_accuracy: 0.5333\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6305 - accuracy: 0.6369 - val_loss: 0.6974 - val_accuracy: 0.4889\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6517 - accuracy: 0.6257 - val_loss: 0.6979 - val_accuracy: 0.5111\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6752 - accuracy: 0.5810 - val_loss: 0.6984 - val_accuracy: 0.5111\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6100 - accuracy: 0.6704 - val_loss: 0.6995 - val_accuracy: 0.5333\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6188 - accuracy: 0.6648 - val_loss: 0.7015 - val_accuracy: 0.5556\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.6563 - accuracy: 0.6480 - val_loss: 0.7028 - val_accuracy: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22cd29a8eb0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet makes predictions on the validation set using the trained LSTM model.\n",
    "\n",
    "The code performs the following steps to make predictions:\n",
    "\n",
    "1. Prediction Probabilities:\n",
    "   - The `model.predict()` method is used to obtain the predicted probabilities for the validation set.\n",
    "   - `X_val` represents the input features of the validation set.\n",
    "   - The predicted probabilities are assigned to the variable `val_probabilities`.\n",
    "\n",
    "2. Thresholding and Prediction Conversion:\n",
    "   - A threshold of `0.5` is applied to the predicted probabilities using `(val_probabilities > 0.5)`. This operation creates a binary array where values above the threshold are `True` and values below or equal to the threshold are `False`.\n",
    "   - The `.astype(int)` method is used to convert the boolean array into integers, resulting in binary predictions.\n",
    "   - The binary predictions are assigned to the variable `val_predictions`.\n",
    "\n",
    "By executing these steps, the code generates predictions on the validation set using the trained LSTM model. The predicted probabilities and binary predictions are obtained and stored in the variables `val_probabilities` and `val_predictions`, respectively.\n",
    "\n",
    "Please note that the code assumes the prior training of the LSTM model and the availability of the validation set (`X_val`) for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 0s/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the validation set\n",
    "val_probabilities = model.predict(X_val)\n",
    "val_predictions = (val_probabilities > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance on the Validation Set\n",
    "\n",
    "The code snippet evaluates the performance of the trained random forest model on the validation set by calculating various evaluation metrics.\n",
    "\n",
    "1. Accuracy:\n",
    "   - The `accuracy_score()` function from the `sklearn.metrics` module is used to calculate the accuracy of the model's predictions on the validation set.\n",
    "   - The `accuracy_score()` function takes the actual target values, `y_val`, and the predicted target values, `val_predictions`, as input and returns the accuracy score.\n",
    "\n",
    "2. Precision:\n",
    "   - The `precision_score()` function is used to calculate the precision of the model's predictions on the validation set.\n",
    "   - The `precision_score()` function takes the actual target values, `y_val`, and the predicted target values, `val_predictions`, as input and returns the precision score.\n",
    "\n",
    "3. Recall:\n",
    "   - The `recall_score()` function is used to calculate the recall of the model's predictions on the validation set.\n",
    "   - The `recall_score()` function takes the actual target values, `y_val`, and the predicted target values, `val_predictions`, as input and returns the recall score.\n",
    "\n",
    "4. F1 Score:\n",
    "   - The `f1_score()` function is used to calculate the F1 score of the model's predictions on the validation set.\n",
    "   - The `f1_score()` function takes the actual target values, `y_val`, and the predicted target values, `val_predictions`, as input and returns the F1 score.\n",
    "\n",
    "5. AUC-ROC Score:\n",
    "   - The `roc_auc_score()` function is used to calculate the AUC-ROC score of the model's predictions on the validation set.\n",
    "   - The `roc_auc_score()` function takes the actual target values, `y_val`, and the predicted target values, `val_predictions`, as input and returns the AUC-ROC score.\n",
    "\n",
    "6. Confusion Matrix:\n",
    "   - The `confusion_matrix()` function is used to calculate the confusion matrix of the model's predictions on the validation set.\n",
    "   - The `confusion_matrix()` function takes the actual target values, `y_val`, and the predicted target values, `val_predictions`, as input and returns the confusion matrix.\n",
    "\n",
    "By executing these steps, the code evaluates the performance of the trained random forest model on the validation set using various evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on the validation set\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "precision = precision_score(y_val, val_predictions)\n",
    "recall = recall_score(y_val, val_predictions)\n",
    "f1 = f1_score(y_val, val_predictions)\n",
    "auc = roc_auc_score(y_val, val_probabilities)\n",
    "confusion = confusion_matrix(y_val, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5333333333333333\n",
      "Precision: 0.5263157894736842\n",
      "Recall: 0.8695652173913043\n",
      "F1 Score: 0.6557377049180327\n",
      "AUC: 0.4861660079051383\n",
      "Confusion Matrix:\n",
      "[[ 4 18]\n",
      " [ 3 20]]\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation metrics and confusion matrix\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'AUC: {auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet makes predictions on the test set using the trained LSTM model and evaluates the model's performance.\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1. Prediction Probabilities:\n",
    "   - The `model.predict()` method is used to obtain the predicted probabilities for the test set.\n",
    "   - `X_test` represents the input features of the test set.\n",
    "   - The predicted probabilities are assigned to the variable `test_probabilities`.\n",
    "\n",
    "2. Thresholding and Prediction Conversion:\n",
    "   - A threshold of `0.5` is applied to the predicted probabilities using `(test_probabilities > 0.5)`. This operation creates a binary array where values above the threshold are `True` and values below or equal to the threshold are `False`.\n",
    "   - The `.astype(int)` method is used to convert the boolean array into integers, resulting in binary predictions.\n",
    "   - The binary predictions are assigned to the variable `test_predictions`.\n",
    "\n",
    "By executing these steps, the code generates predictions on the test set using the trained LSTM model. The predicted probabilities and binary predictions are obtained and stored in the variables `test_probabilities` and `test_predictions`, respectively.\n",
    "\n",
    "The code then proceeds to evaluate the model's performance on the test set using various evaluation metrics:\n",
    "\n",
    "3. Evaluation Metrics:\n",
    "   - The `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, and `roc_auc_score` functions are used to calculate the accuracy, precision, recall, F1 score, and AUC (Area Under the Curve), respectively.\n",
    "   - These evaluation metrics are calculated by comparing the ground truth labels (`y_test`) with the predicted binary labels (`test_predictions`).\n",
    "\n",
    "4. Confusion Matrix:\n",
    "   - The `confusion_matrix` function is used to calculate the confusion matrix, providing a summary of the true positive, false positive, true negative, and false negative predictions.\n",
    "\n",
    "Finally, the evaluation metrics and the confusion matrix are printed to assess the model's performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "\n",
      "Test Set Performance:\n",
      "Test Accuracy: 0.5535714285714286\n",
      "Test Precision: 0.5869565217391305\n",
      "Test Recall: 0.8181818181818182\n",
      "Test F1 Score: 0.6835443037974683\n",
      "Test AUC: 0.47035573122529645\n",
      "Test Confusion Matrix:\n",
      "[[ 4 19]\n",
      " [ 6 27]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "test_probabilities = model.predict(X_test)\n",
    "test_predictions = (test_probabilities > 0.5).astype(int)\n",
    "\n",
    "# Evaluate model performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_precision = precision_score(y_test, test_predictions)\n",
    "test_recall = recall_score(y_test, test_predictions)\n",
    "test_f1 = f1_score(y_test, test_predictions)\n",
    "test_auc = roc_auc_score(y_test, test_probabilities)\n",
    "test_confusion = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Print evaluation metrics and confusion matrix for the test set\n",
    "print('\\nTest Set Performance:')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'Test Precision: {test_precision}')\n",
    "print(f'Test Recall: {test_recall}')\n",
    "print(f'Test F1 Score: {test_f1}')\n",
    "print(f'Test AUC: {test_auc}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
